{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Socrata to Parquet Initial Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Setting up domain of dataset and dataset identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "config = ConfigParser()\n",
    "config.read('./config.ini')\n",
    "app_token = config['socrata']['APP_TOKEN']\n",
    "\n",
    "# Create client to Socrata\n",
    "client = Socrata(domain='data.cityofnewyork.us', app_token=app_token, timeout=60)\n",
    "\n",
    "# NYC 311 Calls (2010-Present)\n",
    "dataset = 'erm2-nwe9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(dataset_identifier: str, query: str, return_df: bool=False) -> pd.DataFrame | list:\n",
    "    '''\n",
    "    Function to query Socrata 'data.cityofnewyork.us' domain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_identifier: name of the dataset\n",
    "    query: SQL-like query string\n",
    "    return_df: Whether to return DataFrame (True) or list of results (False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame or list of results\n",
    "    '''\n",
    "    results = client.get(dataset_identifier=dataset_identifier, query=query)\n",
    "    if return_df:\n",
    "        return pd.DataFrame.from_records(results)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Pull\n",
    "Retrieving the initial dataset, converting datatypes, and writing to Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        unique_key,\n",
    "        created_date,\n",
    "        descriptor,\n",
    "        incident_zip,\n",
    "        community_board,\n",
    "        latitude,\n",
    "        longitude\n",
    "    WHERE\n",
    "        complaint_type = \"Dead Animal\"\n",
    "        AND incident_zip IS NOT NULL\n",
    "        AND community_board IS NOT NULL\n",
    "        AND latitude IS NOT NULL\n",
    "        AND longitude IS NOT NULL\n",
    "    ORDER BY\n",
    "        created_date DESC,\n",
    "        unique_key DESC\n",
    "    LIMIT\n",
    "        20000\n",
    "    \"\"\"\n",
    ")\n",
    "df = get_query(dataset_identifier=dataset, query=query, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>created_date</th>\n",
       "      <th>descriptor</th>\n",
       "      <th>incident_zip</th>\n",
       "      <th>community_board</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56531911</td>\n",
       "      <td>2023-01-15T23:12:46.000</td>\n",
       "      <td>Cat</td>\n",
       "      <td>11206</td>\n",
       "      <td>04 BROOKLYN</td>\n",
       "      <td>40.702889289111376</td>\n",
       "      <td>-73.93339315118288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56534747</td>\n",
       "      <td>2023-01-15T21:23:23.000</td>\n",
       "      <td>Bird</td>\n",
       "      <td>11102</td>\n",
       "      <td>01 QUEENS</td>\n",
       "      <td>40.77271127561967</td>\n",
       "      <td>-73.92181339007267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56535166</td>\n",
       "      <td>2023-01-15T21:19:15.000</td>\n",
       "      <td>Raccoon</td>\n",
       "      <td>11102</td>\n",
       "      <td>01 QUEENS</td>\n",
       "      <td>40.771711019629464</td>\n",
       "      <td>-73.92416850407255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_key             created_date descriptor incident_zip community_board  \\\n",
       "0   56531911  2023-01-15T23:12:46.000        Cat        11206     04 BROOKLYN   \n",
       "1   56534747  2023-01-15T21:23:23.000       Bird        11102       01 QUEENS   \n",
       "2   56535166  2023-01-15T21:19:15.000    Raccoon        11102       01 QUEENS   \n",
       "\n",
       "             latitude           longitude  \n",
       "0  40.702889289111376  -73.93339315118288  \n",
       "1   40.77271127561967  -73.92181339007267  \n",
       "2  40.771711019629464  -73.92416850407255  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Datetypes\n",
    "Converting dates to datetime and latitude/longitude to numeric (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12107 entries, 0 to 12106\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   unique_key       12107 non-null  object\n",
      " 1   created_date     12107 non-null  object\n",
      " 2   descriptor       12107 non-null  object\n",
      " 3   incident_zip     12107 non-null  object\n",
      " 4   community_board  12107 non-null  object\n",
      " 5   latitude         12107 non-null  object\n",
      " 6   longitude        12107 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 662.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datatypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert created date to datetime object\n",
    "    df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "\n",
    "    # Convert latitude and longitude to double\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'])\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_datatypes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12107 entries, 0 to 12106\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   unique_key       12107 non-null  object        \n",
      " 1   created_date     12107 non-null  datetime64[ns]\n",
      " 2   descriptor       12107 non-null  object        \n",
      " 3   incident_zip     12107 non-null  object        \n",
      " 4   community_board  12107 non-null  object        \n",
      " 5   latitude         12107 non-null  float64       \n",
      " 6   longitude        12107 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(4)\n",
      "memory usage: 662.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Parquet and JSON metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy!\n",
    "df.to_parquet(dataset + '.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metadata(current_series: pd.Series, dataset_identifier: str) -> None:\n",
    "    temp = {\n",
    "        'last_date': current_series['created_date'].isoformat(),\n",
    "        'last_pull': datetime.now().isoformat()\n",
    "    }\n",
    "    date_obj = json.dumps(temp, indent=4)\n",
    "    filename = dataset_identifier + '.metadata.json'\n",
    "    with open(filename, 'w') as outfile:\n",
    "        outfile.write(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_metadata(df.iloc[0], dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsequent Data Pulls\n",
    "After the initial data pull, metadata will determine if new 311 calls for dead animals have been made before rewriting a new Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(dataset_identifier: str) -> str:\n",
    "    filename = dataset_identifier + '.metadata.json'\n",
    "    with open(filename, 'r') as file:\n",
    "        metadict = json.load(file)\n",
    "    \n",
    "    return metadict['last_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_date = read_metadata(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01-15T23:12:46'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_complaint_query = (\n",
    "    \"\"\"\n",
    "    SELECT created_date\n",
    "    WHERE\n",
    "        complaint_type = \"Dead Animal\"\n",
    "        AND incident_zip IS NOT NULL\n",
    "        AND community_board IS NOT NULL\n",
    "        AND latitude IS NOT NULL\n",
    "        AND longitude IS NOT NULL\n",
    "    ORDER BY\n",
    "        created_date DESC,\n",
    "        unique_key DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    ")\n",
    "lc_date = get_query(dataset, last_complaint_query)[0]['created_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01-15T23:12:46.000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_pull(metadata_date: str, last_complaint_date: str) -> bool:\n",
    "    md = datetime.fromisoformat(metadata_date)\n",
    "    ld = datetime.fromisoformat(last_complaint_date)\n",
    "\n",
    "    return True if md != ld else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "should_pull(md_date, lc_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible SQL injection but not front facing\n",
    "latest_query = (\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        unique_key,\n",
    "        created_date,\n",
    "        descriptor,\n",
    "        incident_zip,\n",
    "        community_board,\n",
    "        latitude,\n",
    "        longitude\n",
    "    WHERE\n",
    "        created_date > '{}'\n",
    "        AND complaint_type = \"Dead Animal\"\n",
    "        AND incident_zip IS NOT NULL\n",
    "        AND community_board IS NOT NULL\n",
    "        AND latitude IS NOT NULL\n",
    "        AND longitude IS NOT NULL\n",
    "    ORDER BY\n",
    "        created_date DESC,\n",
    "        unique_key DESC\n",
    "    LIMIT\n",
    "        20000\n",
    "    \"\"\".format(md_date)\n",
    ")\n",
    "latest = get_query(dataset, latest_query, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_query(dataset, latest_query)    # Should be empty list since notebook run on same day"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 13:57:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.read.parquet(dataset + '.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unique_key: string (nullable = true)\n",
      " |-- created_date: timestamp (nullable = true)\n",
      " |-- descriptor: string (nullable = true)\n",
      " |-- incident_zip: string (nullable = true)\n",
      " |-- community_board: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+----------+------------+----------+------------+---------------+----------+----------+\n",
      "|unique_key|created_date|descriptor|incident_zip|community_board|  latitude| longitude|\n",
      "+----------+------------+----------+------------+---------------+----------+----------+\n",
      "|  56531911|  2023-01...|       Cat|       11206|     04 BROO...|40.7028...|-73.933...|\n",
      "|  56534747|  2023-01...|      Bird|       11102|      01 QUEENS|40.7727...|-73.921...|\n",
      "|  56535166|  2023-01...|   Raccoon|       11102|      01 QUEENS|40.7717...|-73.924...|\n",
      "|  56533271|  2023-01...|   Opossum|       10305|     02 STAT...|40.6032...|-74.073...|\n",
      "|  56532657|  2023-01...|       Cat|       11229|     15 BROO...|40.6017...|-73.950...|\n",
      "+----------+------------+----------+------------+---------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()\n",
    "new_df.show(n=5, truncate=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-dead-animals-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3 (v3.10.3:a342a49189, Mar 16 2022, 09:34:18) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "968ea6b251939883041292aac5078d5ba2e22bb7b6d32507e861435bf335133b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
